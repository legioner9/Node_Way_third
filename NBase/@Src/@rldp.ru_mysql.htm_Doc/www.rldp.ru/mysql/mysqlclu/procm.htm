<html>
<head>
  <title>MySQL Cluster Manual</title>
  <meta http-equiv="Content-Type" content="text/html; charset=koi8-r>
  <META NAME="GENERATOR" CONTENT="Dos Navigator 1.51.04/DOS.">
  <META NAME="Author" CONTENT="Alexey V. Pautov">
</head>

<body>

<script type="text/javascript">
nN = navigator.appName;
function fsearch(str)
{
  if (document.fform.myradio[0].checked) {
     open('../../../yandex.ru/sitesearch@text=' + str + '&site=' +
          document.location.hostname);
  } else {open('../../../yandex.ru/sitesearch@text=' + str);}
}

function MouseUpHandler(e)
{
  if (nN == 'Netscape' || nN == 'Opera') {
     if (document.getSelection()) {
        str = document.getSelection();
        newstr = str.replace(/\n+/g, ' ');
        str = newstr.replace(/\ +/g, ' ');
        if (str.length > 100) {
           var i;
           i = 0;
           str = str.slice(0, 100);
           i = str.lastIndexOf(' ');
           if (i > 0){str = str.slice(0, i);}
        }
        document.fform.strf.value = str;
     }
  } else if(nN == 'Microsoft Internet Explorer') {
    if (document.selection.createRange()) {
       var range = document.selection.createRange();
       var str = range.text;
    }
    if (str) {
       str = str.replace(/\ +/g, " ");
       document.fform.strf.value = str;
    }
  }
  return true;
}
if (window.Event) {document.captureEvents(Event.MOUSEUP);}
document.onmouseup = MouseUpHandler;
</script>

<center><table><tr><td><a href="../../default.htm">
<IMG SRC="../../img/b_book.gif" ALT="RussianLDP" HEIGHT=48 WIDTH=55
ALIGN=ABSCENTER></a></td>

<td><!--Rating@Mail.ru COUNTEr-->
<a target=_top href="../../../top.mail.ru/jump@from=1364238">
<img src="../../../d1.cd.b4.a1.top.list.ru/counter@id=1364238;t=230"
border=0 height=31 width=88 alt="Рейтинг@Mail.ru"/></a>
</td><!--/COUNTER-->

<td><!-- begin of Top100 code -->
<script id="top100Counter" type="text/javascript"
src="../../../cnt.rambler.ru/top100.jcn@1448139"></script>
<noscript><a href="../../../top100.rambler.ru/top100/default.htm">
<img src="../../../cnt.rambler.ru/top100.cnt@1448139" alt="Rambler's Top100"
width="81" height="63" border="0" /></a></noscript>
<!-- end of Top100 code --></td>

<td><!-- HotLog -->
<script type="text/javascript" language="javascript">
hotlog_js="1.0";
hotlog_r=""+Math.random()+"&s=525943&im=127&r="+escape(document.referrer)+
"&pg="+escape(window.location.href);
document.cookie="hotlog=1; path=/"; hotlog_r+="&c="+(document.cookie?"Y":"N");
</script>
<script type="text/javascript" language="javascript1.1">
hotlog_js="1.1";hotlog_r+="&j="+(navigator.javaEnabled()?"Y":"N")
</script>

<script type="text/javascript" language="javascript1.2">
hotlog_js="1.2";
hotlog_r+="&wh="+screen.width+'x'+screen.height+"&px="+
(((navigator.appName.substring(0,3)=="Mic"))?
screen.colorDepth:screen.pixelDepth)</script>

<script type="text/javascript" language="javascript1.3">
hotlog_js="1.3"</script>
<script type="text/javascript" language="javascript">
hotlog_r+="&js="+hotlog_js;
document.write("<a href='../../../click.hotlog.ru/@525943' target='_top'>
<img "+" src='http://hit27.hotlog.ru/cgi-bin/hotlog/count?"+
hotlog_r+"&' border=0 width=88 height=31 alt=HotLog><\/a>")</script>

<noscript>
<a href="../../../click.hotlog.ru/@525943" target="_top">
<img src="../../../hit27.hotlog.ru/cgi-bin/hotlog/count@s=525943&im=127"
border="0" width="88" height="31" alt="HotLog"></a></noscript></td>
<!-- /HotLog -->

<td><!--LiveInternet counter--><script type="text/javascript"><!--
document.write("<a href='../../../www.liveinternet.ru/click' "+
"target=_blank><img src='http://counter.yadro.ru/hit?t52.15;r"+
escape(document.referrer)+((typeof(screen)=="undefined")?"":
";s"+screen.width+"*"+screen.height+"*"+(screen.colorDepth?
screen.colorDepth:screen.pixelDepth))+";u"+escape(document.URL)+
";"+Math.random()+"' alt='' title='LiveInternet: показано число просмотров и"+
" посетителей за 24 часа' "+"border=0 width=88 height=31><\/a>")//-->
</script></td></tr>
<!--/LiveInternet-->

<tr><td><FONT SIZE=-1><b>WebMoney:&nbsp;</b><br>
WMZ Z294115950220&nbsp;<br>
WMR R409981405661&nbsp;<br>
WME E134003968233&nbsp;</FONT></td>

<td><FONT SIZE=-1><b>Visa&nbsp;</b><br>
4274 3200 2453 6495&nbsp;</FONT></td>
</tr></table></center>
<P><index><table><tr><td>

<H2>4 Управление процессами в MySQL Cluster</H2>
<P>Имеются четыре процесса, которые важны при использовании MySQL Cluster. Мы
здесь разберемся, как работать с этими процессами, какие параметры
использовать при старте и т. д.</P>

<H3>4.1 Использование процесса сервера MySQL для MySQL Cluster</H3>
<P>Традиционным серверным процессом MySQL является <code>mysqld</code>.
Чтобы использоваться с кластером MySQL, он должен быть собран с поддержкой
для способа хранения NDB Cluster. Если <code>mysqld</code> был собран именно
так, то поддержка NDB Cluster по умолчанию выключена.</P>

<P>Чтобы включить поддержку NDB Cluster имеются два способа. Или
использование опции <code>--ndbcluster</code> при запуске
<code>mysqld</code>, встраивание строки <code>ndbcluster</code> в секцию
<code>[mysqld]</code> Вашего файла <tt>my.cnf</tt>. Простой способ проверить,
что Ваш сервер выполняется с поддержкой для <code>NDB Cluster</code> состоит
в том, чтобы выдать команду <code>SHOW ENGINES</code> из клиента
<code>mysql</code>. Вы должны видеть <code>YES</code> для строки
<code>NDBCLUSTER</code>. Если Вы видите <code>NO</code>, Вы не выполняете
<code>mysqld</code>, который компилируется с допускаемой поддержкой
<code>NDB Cluster</code>. Если Вы видите <code>DISABLED</code>, то Вы должны
активизировать поддержку в файле <tt>my.cnf</tt>.</P>

<P>Сервер MySQL должен знать, как получить конфигурацию кластера. Чтобы
обращаться к этой конфигурации, требуется знать три вещи</P>
<UL><LI>Собственный ID узла в кластере.
<LI>Имя хоста (или IP-адрес), где работате сервер управления.
<LI>Порт, на котором можно соединиться с сервером управления.</UL>

<P>ID узла может быть пропущен в MySQL 4.1.5, потому что ID может
быть динамически распределен.</P>

<P>В <code>mysqld</code> применяется параметр <code>ndb-connectstring</code>,
чтобы определить строку подключения connectstring при старте
<code>mysqld</code> или в файле <tt>my.cnf</tt>.</P>
<PRE>
shell&#62; mysqld --ndb-connectstring=ndb_mgmd.mysql.com:1186
</PRE>

<P><code>ndb_mgmd.mysql.com</code> представляет собой хост, на котором
постоянно находится сервер управления, и он слушает порт 1186.</P>

<P>С этой установкой сервер MySQL станет нормальным членом кластера и будет
полностью знать все узлы памяти в кластере и их состояния. Сервер создаст
связи со всеми узлами памяти и будет способен использовать любой узел памяти
как координатор транзакции и обращаться к их данным
для чтения и модифицирования.</P>

<H3>4.2 Процесс <code>ndbd</code> и работа узлов памяти</H3>
<P><code>ndbd</code> представляет собой процесс, который используется, чтобы
обработать все данные в таблицах, использующих NDB Cluster. Это процесс,
который содержит всю логику распределенной обработки транзакции,
восстановления узла, введение контрольных точек на диске, интерактивное
резервирование и большое количество других функциональных возможностей.</P>

<P>В кластере имеется набор процессов <code>ndbd</code>, сотрудничающих в
обработке данные. Эти процессы могут выполняться на том же самом компьютере
или на различных компьютерах способом с
полностью перестраиваемой конфигурацией.</P>

<P>До MySQL 4.1.5 процесс <code>ndbd</code> должен запускаться из отдельного
каталога. Причиной этого было то, что <code>ndbd</code> генерирует набор
журналов в начальном каталоге.</P>

<P>В MySQL 4.1.5 это было изменено так, что файлы помещены в каталог,
определенный <code>DataDir</code> в файле конфигурации. Таким образом,
<code>ndbd</code> может быть запущен отовсюду.</P>

<P>Вот эти журналы (2 обозначает ID узла):</P>
<UL><LI><var>ndb_2_error.log</var> (<tt>error.log</tt> в версии 4.1.3)
файл, который содержит информацию обо всех сбоях, с которыми столкнулся
процесс <code>ndbd</code>, строки сообщений об ошибках и ссылки на файл
трассировки для них, например, так:
<PRE>
Date/Time: Saturday 31 January 2004 - 00:20:01
Type of error: error
Message: Internal program error (failed ndbrequire)
Fault ID: 2341
Problem data: DbtupFixAlloc.cpp
Object of reference: DBTUP (Line: 173)
ProgramName: NDB Kernel
ProcessID: 14909
TraceFile: ndb_2_trace.log.2
***EOM***
</PRE>

<LI><tt>ndb_2_trace.log.1</tt> (<tt>NDB_TraceFile_1.trace</tt> в версии
4.1.3) файл трассировки, описывающий точно, что случилось прежде, чем
произошла ошибка. Эта информация полезна для группы разработки MySQL Cluster
при анализе любых ошибок. Информация в этом файле будет описана в разделе
"<code>Проблемы с MySQL Cluster</code>". Может быть настраиваемое количество
файлов трассировки в этом каталоге. В этом контексте 1 просто номер файла.

<LI><tt>ndb_2_trace.log.next</tt> (<tt>NextTraceFileNo.log</tt> в версии
4.1.3) файл, который следит за тем, каким должен быть следующий номер файла
регистрации, если их несколько.

<LI><tt>ndb_2_out.log</tt> файл, который содержит любые данные, выведенные
процессом <code>ndbd</code>. Этот файл применяется только, если
<code>ndbd</code> запущен в режиме демона, который задан по умолчанию в
версии 4.1.5 и выше. В версии 4.1.3 файл известен как <tt>node2.out</tt>.

<LI><tt>ndb_2.pid</tt> (<tt>node2.pid</tt> в версии 4.1.3) файл, содержащий
идентификатор процесса ndbd при работе в режиме демона. Это также
функционирует как файл блокировки, чтобы избежать запуска узлов с тем
же самым идентификатором.

<LI><tt>ndb_2_signal.log</tt> (был <tt>Signal.log</tt> в версии 4.1.3)
файл, который используется только в отладочных версиях <code>ndbd</code>,
где возможно проследить все входящие, исходящие и внутренние сообщения с их
данными в процессе <code>ndbd</code>.</UL>

<P>Рекомендуется не использовать каталог, доступный через NFS, потому что в
некоторых средах могут быть проблемы с блокировкой pid-файла, остающегося
даже после завершения процесса.</P>

<P>Также при старте процесса <code>ndbd</code> может быть необходимо
определить имя сервера управления и порта, который слушает подключения,
факультативно можно определять ID узла, который процесс должен использовать.
<PRE>
shell&#62; ndbd --connect-string="nodeid=2;host=ndb_mgmd.mysql.com:1186"
</PRE></P>

<P>При запуске <code>ndbd</code> реально запускается пара процессов.
Первый присматривает за вторым и перезапускает его в случае сбоя. Так что
Unix-командой <code>kill</code> надлежит прерывать оба процесса, а лучше
всего использовать клиент управления и завершать <code>ndbd</code> оттуда.
</P>

<P>Процесс выполнения использует один поток для всех действий чтения, записи,
просмотра данных и всех других действий. Этот поток разработан с асинхронным
программированием, так что это может легко обрабатывать тысячи параллельных
задач. Кроме того, имеется поток сторожа, контролирующий поток выполнения,
чтобы гарантировать, что он не останавливается в вечном цикле или другой
проблеме. Имеется пул потоков для обслуживания файлового ввода-вывода. Каждый
из этих потоков может обрабатывать один открытый файл. Кроме того, потоки
могут использоваться для действий подключения транспортеров в процессе
<code>ndbd</code>. Таким образом, в системе, которая выполняет большое
количество действий, включая действия модификации, процесс <code>ndbd</code>
потребит приблизительно до 2 CPU, если это позволено. Таким образом, в
большом SMP-блоке со многими CPU рекомендуется использовать несколько
процессов <code>ndbd</code>, которые сконфигурированы так, чтобы быть частью
различных групп узлов.</P>

<H3>4.3 <code>ndb_mgmd</code>, процесс сервера управления</H3>
<P>Сервер управления представляет собой процесс, который читает файл
конфигурации кластера и распределяет эту информацию по всем узлам в кластере,
запрашивающих ее. Это также поддерживает файл регистрации действий кластера.
Клиентура управления может соединяться с сервером управления и использовать
команды, чтобы проверить состояние кластера в различных аспектах.</P>

<P>Начиная с MySQL 4.1.5, нет надобности определять строку подключения при
запуске управляющего сервера. Однако, если Вы используете несколько серверов
управления, нужно обеспечить эту строку, а каждый узел в кластере должен
определить свой nodeid явно.</P>

<P>Следующие файлы созданы или используются <code>ndb_mgmd</code> в начальном
каталоге <code>ndb_mgmd</code>. Начиная с MySQL 4.1.5, файлы протоколов и PID
будут помещены в <code>DataDir</code>, определенный в файле конфигурации:</P>

<UL><LI><tt>config.ini</tt> представляет собой файл конфигурации кластера.
Он создан пользователем и читается сервером управления. Как писать этот файл
рассказано в разделе "<code>Настройка MySQL Cluster</code>".

<LI><tt>ndb_1_cluster.log</tt> (<tt>cluster.log</tt> в версии 4.1.3) файл,
где фиксируются события в кластере. Примеры событий: контрольные точки,
(начатые и завершенные), сбои узлов, уровни использования памяти и т.д.
События описаны в разделе
"<A HREF="MySQL_Cluster_Management.html">5 Управление кластером MySQL</A>".

<LI><tt>ndb_1_out.log</tt> (<tt>node1.out</tt> в версии 4.1.3) файл,
используемый для stdout и stderr при выполнении сервера управления как
демона. В этом контексте 1 задает ID узла.

<LI><tt>ndb_1.pid</tt> (<tt>node1.pid</tt> в версии 4.1.3) PID-файл,
используемый при выполнении сервера управления как демона.
В этом контексте 1 задает ID узла.

<LI><tt>ndb_1_cluster.log.1</tt> (<tt>cluster.log.1</tt> в версии 4.1.3)
когда файл регистрации кластера становится большим, чем один миллион байт
(именно миллион байт, а не мегабайт!), файл переименован к этому имени. Здесь
1 номер журнала кластера, такчто , если 1, 2 и 3 уже существуют следующий
будет иметь номер 4.</UL>

<H3>4.4 <code>ndb_mgm</code>, клиентский процесс управления</H3>
<P>Последний важный процесс, о котором надо знать, клиент управления. Этот
процесс не так уж необходим, чтобы выполнить кластер. Он позволяет управлять
кластером через сервер управления с помощью ряда команд.</P>

<P>Фактически клиент управления использует C API, который используется, чтобы
обратиться к серверу управления, так что для продвинутых пользователей также
возможно написать свою программу для управления кластером.</P>

<P>При старте управляющего клиента, необходимо установить имя хоста и порт
для связи с сервером управления как в примере ниже. Значение по умолчанию:
localhost и порт 1186 (был 2200 до версии 4.1.8).</P>
<PRE>
shell&#62; ndb_mgm localhost 1186
</PRE>

<A name="param"><H3>4.5 Параметры команд для MySQL Cluster</H3>
<P>Все исполняемые модули в MySQL Cluster (кроме <code>mysqld</code>)
понимают следующие параметры, начиная с версии 4.1.8. Если вы выполняете
более раннюю версию, внимательно ознакомьтесь с текстом ниже, там указаны
все отличия и проблемы. Обратите внимание также, что Вы можете использовать
опцию <code>-?</code>, чтобы увидеть, что обеспечивается в Вашей версии.</P>

<DL COMPACT><DT><code>-?, --usage, --help</code>
<DD>Печатает короткое описание доступных параметров команды.

<DT><code>-V, --version</code>
<DD>Печатает номер версии процесса <code>ndbd</code>. Это номер версии MySQL
Cluster. Важно, потому что при запуске процессы MySQL Cluster проверяют,
какие версии процессов могут сосуществовать в кластере. Это также важно для
интерактивного программного обновления MySQL Cluster.

<DT><code>-c <var>connect_string</var> (не <code>ndb_mgmd</code>),
--connect-string <var>connect_string</var></code>
<DD>Установить строку подключения к серверу управления как опцию команды
(по причинам обратной совместимости <code>ndb_mgmd</code> до версии 5.0 опцию
-c не обрабатывает, поскольку она в настоящее время определяет файл
конфигурации). Доступно с <code>ndb_mgm</code> 4.1.8 и выше.
<PRE>
shell&#62; ndbd --connect-string="nodeid=2;host=ndb_mgmd.mysql.com:1186"
</PRE>

<DT><code>--debug[=<var>options</var>]</code>
<DD>Это может использоваться только для версий с информацией отладки. Это
используется, чтобы допустить вывод из вызовов отладки тем же самым способом,
что и у процесса <code>mysqld</code>.</DL>

<H4>4.5.1 MySQL Cluster-специфичные параметры для <code>mysqld</code></H4>
<DL COMPACT><DT><code>--ndbcluster</code>
<DD>Если двоичный модуль включает поддержку для способа хранения <code>NDB
Cluster</code>, эта опция активизирует данную поддержку, а она необходима для
работы MySQL Cluster.

<DT><code>--skip-ndbcluster</code>
<DD>Выключает поддержку <code>NDB Cluster</code>. По умолчанию поддержка и
так выключена, так что эта опция нужна лишь на серверах, заранее
сконфигурированных для кластерной работы.

<DT><code>--ndb-connectstring=<var>connect_string</var></code>
<DD>При использовании <code>NDB</code>, возможно подчеркнуть сервер
управления, который распределяет конфигурацию кластера, устанавливая
опцию строки подключения.</DL>

<H4>4.5.2 Опции для <code>ndbd</code></H4>
<P>Основные параметры перечислены в разделе
"<A HREF="#param">4.5 Параметры команд для MySQL Cluster</A>".</P>

<DL COMPACT><DT><code>-d, --daemon</code>
<DD>Инструктирует <code>ndbd</code> выполниться как процесс-демон. В MySQL
4.1.5 и выше включено по умолчанию.

<DT><code>--nodaemon</code>
<DD>Инструктирует <code>ndbd</code> не выполняться как процесс-демон.
Полезно, когда <code>ndbd</code> отлаживается и выводит на экран отчеты.

<DT><code>--initial</code>
<DD>Инструктирует <code>ndbd</code> выполнить начальную инициализацию. Это
сотрет любые файлы, созданные ранее <code>ndbd</code> для восстановления. Это
также вновь создаст журналы восстановления, что на некоторых операционных
системах может занимать немалое время. Инициализация применяется только при
самом первом запуске процесса <code>ndbd</code>. Это удаляет все служебные
файлы из файловой системы и создает все REDO-протоколы. При выполнении
программного обновления, которое изменило содержание любых файлов, также
необходимо использовать эту опцию при перезапуске узла с новой программной
версией <code>ndbd</code>. В заключение это могло бы также использоваться как
последний аргумент для узла, который невозможно перезапустить. В этом случае
помните, что разрушение содержания файловой системы означает, что этот узел
больше не может использоваться, чтобы восстановить данные. Эта опция не
воздействует на любые созданные файлы с резервными копиями. Ранее имевшаяся
возможность использовать <code>-i</code> для этой опции была удалена, чтобы
гарантировать, что эта опция не используется по ошибке.

<DT><code>--nostart</code>
<DD>Инструктирует <code>ndbd</code> не запускаться автоматически. Процесс
<code>ndbd</code> соединится с сервером управления, получит конфигурацию и
инициализирует объекты связи. Это не будет запускать основные процессы, пока
не получит соответствующую инструкцию сервера управления в явном виде. На
деле такая инструкция обычно выдается через сервер клиентом управления.</DL>

<H4>4.5.3 Опции для <code>ndb_mgmd</code></H4>
<P>Основные параметры перечислены в разделе
"<A HREF="param">4.5 Параметры команд для MySQL Cluster</A>".</P>

<DL COMPACT><DT><code>-f <var>filename</var> (from 4.1.8),
--config-file=<var>filename</var>, -c <var>filename</var>
(устарело, начиная с версии 5.0)</code>
<DD>Эта опция должна быть определена в обязательном порядке. Она определяет,
какой именно файл будет рассматриваться как конфигурационный. По умолчанию
это <code>config.ini</code>.

<DT><code>-d, --daemon</code>
<DD>Инструктирует <code>ndb_mgmd</code> выполниться как процесс-демон.
Задано по умолчанию.

<DT><code>-nodaemon</code>
<DD>Инструктирует <code>ndb_mgmd</code> не выполняться как процесс-демон.
</DL>

<H4>4.5.4 Опции для <code>ndb_mgm</code></H4>
<P><DL COMPACT><DT><code>[<var>host_name</var> [<var>port_num</var>]]</code>
<DD>Чтобы запустить клиент управления, необходимо определить, где сервер
управления постоянно находится. Это означает, что надо определять имя
машины и порт. Значение по умолчанию <code>localhost</code> и порт 1186
(был 2200 до версии 4.1.8).

<DT><code>--try-reconnect=<var>number</var></code>
<DD>Если подключение прервано, можно выполнить только определенное количество
повторных попыток соединиться перед сообщением пользователю кода
неисправности. Значение по умолчанию: повторять до успеха каждые 5 секунд.
</DL>

<A name="manag"><H2>5 Управление MySQL Cluster</H2>
<P>Управление MySQL Cluster включает ряд действий. Первое действие должно
конфигурировать и запустить MySQL Cluster.</P>

<P>Имеются по существу два способа активного управления выполнением MySQL
Cluster. Первый: командами, введенными в клиенте управления, где состояние
кластера может быть проверено. Второй метод состоит в том, чтобы исследовать
вывод в файле регистрации кластера. Файл регистрации кластера направлен в
<tt>ndb_2_cluster.log</tt> в каталоге <code>DataDir</code> сервера
управления. Файл регистрации содержит отчеты о событиях, сгенерированные из
процессов <code>ndbd</code> в кластере. Также возможно послать записи файла
регистрации кластера файлу регистрации системы Unix.</P>

<H3>5.1 Команды в клиенте управления</H3>
<P>В дополнение к центральному файлу конфигурации, кластер может также
управляться через интерфейс командной строки. Интерфейс командной строки
доступен через отдельный процесс клиента управления. Это основной
административный интерфейс к выполняющемуся кластеру.</P>

<P>Клиент  управления имеет следующие базисные команды. Ниже
<code>&#60;id&#62;</code> обозначает идентификатор узла базы данных
(например, 21) или ключевое слово <code>ALL</code>, что указывает, что
команда должна применяться ко всем узлам баз данных в кластере.</P>

<DL COMPACT><DT><code>HELP</code>
<DD>Печатает информацию о всех доступных командах.

<DT><code>SHOW</code>
<DD>Печатает информацию относительно состояния кластера.

<DT><code>&#60;id&#62; START</code>
<DD>Запустит узел базы данных, идентифицированный <code>&#60;id&#62;</code>,
или все узлы баз данных.

<DT><code>&#60;id&#62; STOP</code>
<DD>Выключит узел базы данных, идентифицированный <code>&#60;id&#62;</code>,
или все узлы баз данных.

<DT><code>&#60;id&#62; RESTART [-N] [-I]</code>
<DD>Перезапустит узел базы данных, идентифицированный
<code>&#60;id&#62;</code>, или все узлы баз данных.

<DT><code>&#60;id&#62; STATUS</code>
<DD>Информация состояния для узла базы данных, идентифицированного
<code>&#60;id&#62;</code>, или для всех (<code>ALL</code>) узлов.

<DT><code>ENTER SINGLE USER MODE &#60;id&#62;</code>
<DD>Вводит режим отдельного пользователя, где только API с идентификатором
<code>&#60;id&#62;</code> позволяют обратиться к системе базы данных.

<DT><code>EXIT SINGLE USER MODE</code>
<DD>Отменяет режим отдельного пользователя, позволяя всем API обратиться к
системе базы данных.

<DT><code>QUIT</code>
<DD>Завершает клиент управления..

<DT><code>SHUTDOWN</code>
<DD>Завершает работу всех узлов кластера (за исключением серверов mysql) и
выходит из клиента управления.</DL>

<P>Команды для файлов регистрации событий даны в следующем разделе, команды
для копирования и восстановления даны в отдельных разделах по этим темам.</P>

<H3>5.2 Отчеты о событиях, сгенерированные в MySQL Cluster</H3>
<P>MySQL Cluster имеет два файла регистрации событий: файл регистрации
кластера и файл регистрации узла.</P>

<UL><LI>Файл регистрации кластера ведет протокол работы целого кластера, и
этот файл регистрации может иметь многократных адресатов (файл, консоль
сервера управления или syslog).

<LI>Файл регистрации узла является локальным для каждого узла базы данных и
записан на консоль узла базы данных. Два файла регистрации могут быть
установлены, чтобы регистрировать различные подмножества списка событий.</UL>

<P>Примечание: файл регистрации кластера является рекомендуемым. Файл
регистрации узла предназначен только, чтобы использоваться в течение
разработки прикладной программы или для отладки кода прикладной программы.
</P>

<P>Каждое событие имеет следующие реквизиты:
<UL><LI>Категория (STARTUP, SHUTDOWN, STATISTICS, CHECKPOINT, NODERESTART,
CONNECTION, ERROR, INFO).
<LI>Приоритет (1-15, где 1 наиболее важный, 15 наименее важный).
<LI>Серьезность (ALERT, CRITICAL, ERROR, WARNING, INFO, DEBUG).</UL>

<P>Два файла регистрации (файл регистрации кластера и файл регистрации узла)
могут быть фильтрованы, основываясь на этих реквизитах.</P>

<H4>5.2.1 Команды управления протоколированием</H4>
<P>Следующие команды управления связаны с файлом регистрации кластера:</P>

<DL COMPACT><DT><code>CLUSTERLOG ON</code>
<DD>Включить протоколирование кластера.

<DT><code>CLUSTERLOG OFF</code>
<DD>Выключить протоколирование кластера.

<DT><code>CLUSTERLOG INFO</code>
<DD>Информация относительно параметров настройки протоколирования.

<DT><code>&#60;id&#62; CLUSTERLOG &#60;category&#62;=&#60;threshold&#62;
</code>
<DD>События категории category с приоритетом меньше, чем или равным threshold
отмечаются в файле регистрации кластера.

<DT><code>CLUSTERLOG FILTER &#60;severity&#62;</code>
<DD>Регистрация событий кластера определенного типа серьезности вкл\выкл.
</DL>

<P>Следующая таблица описывает настройку по умолчанию (для всех узлов базы
данных) порога категории файла регистрации кластера. Если событие имеет
приоритет со значением ниже, чем или равным порогу приоритета, то это будет
обязательно сообщено в файле регистрации кластера.</P>

<P>Обратите внимание, что события сообщены узлом базы данных, и что пороги
могут быть установлены по-разному на различных узлах.</P>

<TABLE><TR><TD><strong>Категория</strong></TD><TD><strong>Порог по умолчанию
(все узлы базы данных)</strong></TD></TR>
<TR><TD>STARTUP</TD><TD>7</TD></TR>
<TR><TD>SHUTDOWN</TD><TD>7</TD></TR>
<TR><TD>STATISTICS</TD><TD>7</TD></TR>
<TR><TD>CHECKPOINT</TD><TD>7</TD></TR>
<TR><TD>NODERESTART</TD><TD>7</TD></TR>
<TR><TD>CONNECTION</TD><TD>7</TD></TR>
<TR><TD>ERROR</TD><TD>15</TD></TR>
<TR><TD>INFO</TD><TD>7</TD></TR></TABLE>

<P>Порог используется, чтобы фильтровать события внутри каждой категории.
Например: событие <code>STARTUP</code> с приоритетом 3 никогда не будет
послано, если порог для <code>STARTUP</code> не установлен в 3 или ниже.
Только события с приоритетом 3 или ниже будут посланы, если порог равен 3.
Теперь о серьезности событий (она соответствует уровням UNIX syslog):</P>

<TABLE><TR><TD>1</TD><TD>ALERT</TD><TD>Ситупция, которая должна быть
исправлена немедленно, типа разрушенной базы данных</TD></TR>
<TR><TD>2</TD><TD>CRITICAL</TD><TD>Критическая ситуация, типа ошибок
устройства или исчерпания ресурсов</TD></TR>
<TR><TD>3</TD><TD>ERROR</TD><TD>Проблема, которая должна быть исправлена,
типа ошибок конфигурации</TD></TR>
<TR><TD>4</TD><TD>WARNING</TD><TD>Это не ошибка, но может требовать обработки
</TD></TR>
<TR><TD>5</TD><TD>INFO</TD><TD>Информационные сообщения</TD></TR>
<TR><TD>6</TD><TD>DEBUG</TD><TD>Сообщения, используемые в ходе разработки и
отладки NDB Cluster</TD></TR></TABLE>

<P>Коды <code>LOG_EMERG</code> и <code>LOG_NOTICE</code> из syslog здесь не
применяются и не поддерживаются.</P>

<P>Серьезность событий (event severity) может быть включена или выключена. От
этого зависит, будут ли регистрироваться события соответствующей серьезности.
Если серьезность включена, регистрируются все события с приоритетом меньше,
чем или равным порогу категории. Если серьезность выключена, не будет
отмечено никаких событий, принадлежащих к этой серьезности.</P>

<H4>5.2.2 Сообщения в протоколах событий</H4>
<P>Все регистрируемые события перечислены ниже.</P>
<TABLE><TR><TD><strong>Событие</strong></TD>
<TD><strong>Категория (Category)</strong></TD>
<TD><strong>Приоритет (Priority)</strong></TD>
<TD><strong>Серьезность (Severity)</strong></TD>
<TD><strong>Описание</strong></TD></TR>
<TR><TD>DB nodes connected</TD><TD>CONNECTION</TD><TD>8</TD><TD>INFO</TD>
<TD>Узел базы данных подключился</TD></TR>
<TR><TD>DB nodes disconnected</TD><TD>CONNECTION</TD><TD>8</TD><TD>INFO</TD>
<TD>Узел базы данных отключился</TD></TR>
<TR><TD>Communication closed</TD><TD>CONNECTION</TD><TD>8</TD><TD>INFO</TD>
<TD>Соединение узлов API &#38; DB закрыто</TD></TR>
<TR><TD>Communication opened</TD><TD>CONNECTION</TD><TD>8</TD><TD>INFO</TD>
<TD>Соединение узлов API &#38; DB открыто</TD></TR>
<TR><TD>Global checkpoint started</TD><TD>CHECKPOINT</TD><TD>9</TD>
<TD>INFO</TD><TD>Начало GCP, то есть REDO-протокол записан на диск</TD></TR>
<TR><TD>Global checkpoint completed</TD><TD>CHECKPOINT</TD><TD>10</TD>
<TD>INFO</TD><TD>GCP завершена</TD></TR>
<TR><TD>Local checkpoint started</TD><TD>CHECKPOINT</TD><TD>7</TD>
<TD>INFO</TD><TD>Начало локальной контрольной точки, то есть данные записаны
на диск. LCP Id и GCI Id хранятся и восстанавливается самый старый</TD></TR>
<TR><TD>Local checkpoint completed</TD><TD>CHECKPOINT</TD><TD>8</TD>
<TD>INFO</TD><TD>LCP завершена</TD></TR>
<TR><TD>LCP stopped in calc keep GCI</TD><TD>CHECKPOINT</TD><TD>0</TD>
<TD>ALERT</TD><TD>LCP прервана!</TD></TR>
<TR><TD>Local checkpoint fragment completed</TD><TD>CHECKPOINT</TD>
<TD>11</TD><TD>INFO</TD><TD>LCP на фрагменте завершена</TD></TR>
<TR><TD>Report undo log blocked</TD><TD>CHECKPOINT</TD><TD>7</TD>
<TD>INFO</TD><TD>Буфер протоколирования отмен (undo) скоро переполнится</TD>
</TR>
<TR><TD>DB node start phases initiated</TD><TD>STARTUP</TD><TD>1</TD>
<TD>INFO</TD><TD>Инициализирован NDB Cluster</TD></TR>
<TR><TD>DB node all start phases completed</TD><TD>STARTUP</TD><TD>1</TD>
<TD>INFO</TD><TD>Запущен NDB Cluster</TD></TR>
<TR><TD>Internal start signal received STTORRY</TD><TD>STARTUP</TD>
<TD>15</TD><TD>INFO</TD><TD>Внутренний стартовый сигнал блокам
после законченного рестарта</TD></TR>
<TR><TD>DB node start phase X completed</TD><TD>STARTUP</TD><TD>4</TD>
<TD>INFO</TD><TD>Стартовая фаза завершена</TD></TR>
<TR><TD>Node has been successfully included into the cluster</TD>
<TD>STARTUP</TD><TD>3</TD><TD>INFO</TD><TD>Узел успешно включен в кластер
</TD></TR>
<TR><TD>Node has been refused to be included into the cluster</TD>
<TD>STARTUP</TD><TD>8</TD><TD>INFO</TD><TD>Узел не удалось включить в кластер
</TD></TR>
<TR><TD>DB node neighbours</TD><TD>STARTUP</TD><TD>8</TD><TD>INFO</TD>
<TD>Показывает соседние узлы базы данных</TD></TR>
<TR><TD>DB node shutdown initiated</TD><TD>STARTUP</TD><TD>1</TD>
<TD>INFO</TD><TD>Инициализировано закрытие системы узла</TD></TR>
<TR><TD>DB node shutdown aborted</TD><TD>STARTUP</TD><TD>1</TD>
<TD>INFO</TD><TD>Прервано закрытие системы узла</TD></TR>
<TR><TD>New REDO log started</TD><TD>STARTUP</TD><TD>10</TD><TD>INFO</TD>
<TD>GCI хранит X, самый новый восстанавливаемый GCI Y</TD></TR>
<TR><TD>New log started</TD><TD>STARTUP</TD><TD>10</TD><TD>INFO</TD>
<TD>Часть файла регистрации X, начата MB Y, завершена MB Z</TD></TR>
<TR><TD>Undo records executed</TD><TD>STARTUP</TD><TD>15</TD><TD>INFO</TD>
<TD>Выполнение записи отмены (Undo)</TD></TR>
<TR><TD>Completed copying of dictionary information</TD><TD>NODERESTART</TD>
<TD>8</TD><TD>INFO</TD><TD>Завершено копирование информации словаря</TD></TR>
<TR><TD>Completed copying distribution information</TD><TD>NODERESTART</TD>
<TD>8</TD><TD>INFO</TD><TD>Завершено копирование информации распределения
</TD></TR>
<TR><TD>Starting to copy fragments</TD><TD>NODERESTART</TD><TD>8</TD>
<TD>INFO</TD><TD>Начало копирования фрагментов</TD></TR>
<TR><TD>Completed copying a fragment</TD><TD>NODERESTART</TD><TD>10</TD>
<TD>INFO</TD><TD>Завершение копирования фрагментов</TD></TR>
<TR><TD>Completed copying all fragments</TD><TD>NODERESTART</TD><TD>8</TD>
<TD>INFO</TD><TD>Завершение копирования ВСЕХ фрагментов</TD></TR>
<TR><TD>Node failure phase completed</TD><TD>NODERESTART</TD><TD>8</TD>
<TD>ALERT</TD><TD>Завершена ситуация сбоя узла</TD></TR>
<TR><TD>Node has failed, node state was X</TD><TD>NODERESTART</TD><TD>8</TD>
<TD>ALERT</TD><TD>Узел начал сбоить</TD></TR>
<TR><TD>Report whether an arbitrator is found or not</TD><TD>NODERESTART</TD>
<TD>6</TD><TD>INFO</TD><TD>7 различных ситуаций</TD></TR>
<TR><TD></TD><TD></TD><TD></TD><TD></TD><TD>- Координатор перезапускает поток
арбитража [состояние=X]</TD></TR>
<TR><TD></TD><TD></TD><TD></TD><TD></TD><TD>- Подготовка узла арбитратора X
[ticket=Y]</TD></TR>
<TR><TD></TD><TD></TD><TD></TD><TD></TD><TD>- Принять узел арбитратора X
[ticket=Y]</TD></TR>
<TR><TD></TD><TD></TD><TD></TD><TD></TD><TD>- Запустить узел арбитратора X
[ticket=Y]</TD></TR>
<TR><TD></TD><TD></TD><TD></TD><TD></TD><TD>- Потерян узел арбитратора
X: обрабатывает сбой [состояние=Y]</TD></TR>
<TR><TD></TD><TD></TD><TD></TD><TD></TD><TD>- Потерян узел арбитратора
X: обрабатывает выход [состояние=Y]</TD></TR>
<TR><TD></TD><TD></TD><TD></TD><TD></TD><TD>- Потерян узел арбитратора
X &#60;сообщение_об_ошибке&#62;[состояние=Y]</TD></TR>
<TR><TD>Report arbitrator results</TD><TD>NODERESTART</TD><TD>2</TD>
<TD>ALERT</TD><TD>8 разных результатов</TD></TR>
<TR><TD></TD><TD></TD><TD></TD><TD></TD><TD>- Потерянная проверка арбитража:
меньше половины узлов доступны</TD></TR>
<TR><TD></TD><TD></TD><TD></TD><TD></TD><TD>- Проверка арбитража: узлы
сгруппированы как надо</TD></TR>
<TR><TD></TD><TD></TD><TD></TD><TD></TD><TD>- Потерянная проверка Арбитража:
отсутствующая группа узла</TD></TR>
<TR><TD></TD><TD></TD><TD></TD><TD></TD><TD>- Выделение разделов сети по
требованию арбитража</TD></TR>
<TR><TD></TD><TD></TD><TD></TD><TD></TD><TD>- Положительный ответ из узла X
</TD></TR>
<TR><TD></TD><TD></TD><TD></TD><TD></TD><TD>- Арбитраж потерян: отрицательный
ответ из узла X</TD></TR>
<TR><TD></TD><TD></TD><TD></TD><TD></TD><TD>- Разрыв сети: никакой арбитратор
сейчас не доступен</TD></TR>
<TR><TD></TD><TD></TD><TD></TD><TD></TD><TD>- Разрыв сети: нет
конфигурации для арбитратора</TD></TR>
<TR><TD>GCP take over started</TD><TD>NODERESTART</TD><TD>7</TD>
<TD>INFO</TD><TD>GCP запущен</TD></TR>
<TR><TD>GCP take over completed</TD><TD>NODERESTART</TD><TD>7</TD>
<TD>INFO</TD><TD>GCP завершен</TD></TR>
<TR><TD>LCP take over started</TD><TD>NODERESTART</TD><TD>7</TD>
<TD>INFO</TD><TD>LCP запущен</TD></TR>
<TR><TD>LCP take completed (state = X)</TD><TD>NODERESTART</TD><TD>7</TD>
<TD>INFO</TD><TD>LCP завершен с состоянием X</TD></TR>
<TR><TD>Report transaction statistics</TD><TD>STATISTICS</TD><TD>8</TD>
<TD>INFO</TD><TD>Количество транзакций, завершений транзакции, чтений,
простых чтений, записей, параллельных операций, работ с информацией
атрибутов, аварийных прекращений работы</TD></TR>
<TR><TD>Report operations</TD><TD>STATISTICS</TD><TD>8</TD><TD>INFO</TD>
<TD>Число проделанных операций</TD></TR>
<TR><TD>Report table create</TD><TD>STATISTICS</TD><TD>7</TD>
<TD>INFO</TD><TD>Создана таблица отчета</TD></TR>
<TR><TD>Report job scheduling statistics</TD><TD>STATISTICS</TD><TD>9</TD>
<TD>INFO</TD><TD>Означает внутреннюю работу, планирующую статистику</TD></TR>
<TR><TD>Sent # of bytes</TD><TD>STATISTICS</TD><TD>9</TD><TD>INFO</TD>
<TD>Среднее количество байт, посланных узлу X</TD></TR>
<TR><TD>Received # of bytes</TD><TD>STATISTICS</TD><TD>9</TD>
<TD>INFO</TD><TD>Среднее количество байт, полученных от узла X</TD></TR>
<TR><TD>Memory usage</TD><TD>STATISTICS</TD><TD>5</TD><TD>INFO</TD>
<TD>Использование памяти данными и индексом (80%, 90% и 100%)</TD></TR>
<TR><TD>Transporter errors</TD><TD>ERROR</TD><TD>2</TD><TD>ERROR</TD>
<TD>Ошибки транспортера</TD></TR>
<TR><TD>Transporter warnings</TD><TD>ERROR</TD><TD>8</TD>
<TD>WARNING</TD><TD>Предупреждения транспортера</TD></TR>
<TR><TD>Missed heartbeats</TD><TD>ERROR</TD><TD>8</TD><TD>WARNING</TD>
<TD>Узел X пропустил синхронизацию Y</TD></TR>
<TR><TD>Dead due to missed heartbeat</TD><TD>ERROR</TD><TD>8</TD>
<TD>ALERT</TD><TD>Узел X объявлен зависшим по причине пропуска синхронизации
</TD></TR>
<TR><TD>General warning events</TD><TD>ERROR</TD><TD>2</TD><TD>WARNING</TD>
<TD>Общие события предупреждений</TD></TR>
<TR><TD>Sent heartbeat</TD><TD>INFO</TD><TD>12</TD><TD>INFO</TD>
<TD>Синхронизация послана узлу X</TD></TR>
<TR><TD>Create log bytes</TD><TD>INFO</TD><TD>11</TD><TD>INFO</TD>
<TD>Часть файла регистрации, имя файла, его размер в MB</TD></TR>
<TR><TD>General info events</TD><TD>INFO</TD><TD>2</TD><TD>INFO</TD>
<TD>Общие события информации</TD></TR></TABLE>

<P>Отчет события имеет следующий формат в файлах регистрации:
<PRE>
&#60;Дата &#38; время в GMT&#62; [&#60;строка&#62;]
&#60;серьезность события&#62; -- &#60;сообщение файла регистрации&#62;
</PRE>

<P>Пример отчета о событии:
<PRE>
09:19:30 2003-04-24 [NDB] INFO -- Node 4 Start phase 4 completed
</PRE>

<H3>5.3 Однопользовательский режим работы</H3>
<P>Режим отдельного пользователя (он же однопользовательский) позволяет
администратору базы данных ограничивать доступ к системе базы данных только
одной прикладной программой (узлом API). При вводе режима отдельного
пользователя все подключения всех API будут элегантно закрыты, и никакие
транзакции не могут быть начаты (но могут быть завершены).</P>

<P>Когда кластер вошел в режим отдельного пользователя, доступ к базе данных
предоставлен только определенному узлу API. Пример:
<PRE>
ENTER SINGLE USER MODE 5
</PRE>

<P>После выполнения этой команды, узел API с идентификатором узла 5
становится единственным пользователем кластера.</P>

<P>Узел, определенный в команде, выше должен быть узлом сервера MySQL.
Любая попытка определять любой другой тип узла будет отклонена.</P>

<P>Обратите внимание: если узел с идентификатором 5 выполняется, когда
выполнена команда <code>ENTER SINGLE USER MODE 5</code>, все транзакции,
выполняющиеся на узле 5, будут прерваны, подключение закрыто, а сервер
должен быть перезапущен.</P>

<P>Команда <code>EXIT SINGLE USER MODE</code> изменяет состояние кластера на
многопользовательский режим работы. Серверам MySQL, ждущим подключения,
теперь разрешают соединиться. Сервер MySQL, обозначенный как отдельный
пользователь, продолжает выполняться, если он подключен к кластеру в течение
и после изменения его состояния. Пример:
<PRE>
EXIT SINGLE USER MODE
</PRE>

<P>Лучше всего в случае сбоев узла при выполнении в
режиме отдельного пользователя:</P>
<OL><LI>Закончить все транзакции режима отдельного пользователя.
<LI>Выйти из режима отдельного пользователя.
<LI>Перезапустить узлы базы данных.</OL>

<P>Или перезапустите узлы базы данных до ввода
режима отдельного пользователя.</P>

<H3>5.4 Резервирование MySQL Cluster</H3>
<P>Этот раздел описывает, как создать резервную копию и позже восстановить
из нее базу данных.</P>

<H4>5.4.1 Концепция резервирования кластера</H4>
<P>Копия представляет собой снимок (образ) базы данных в данное время.
Копия содержит три основных части:</P>

<OL><LI>Метаданные (какие таблицы существуют и т. п.).
<LI>Записи таблиц (данные в таблицах).
<LI>Файл регистрации совершенных транзакций.</OL>

<P>Каждая из этих частей сохранена на всех узлах, участвующих в копировании.
</P>

<P>В течение резервирования каждый узел сохраняет эти три части на
диск в три файла:</P>

<DL COMPACT><DT><tt>BACKUP-&#60;BackupId&#62;.&#60;NodeId&#62;.ctl</tt>
<DD>Управляющий файл, который содержат управляющую информацию и метаданные.

<DT><tt>BACKUP-&#60;BackupId&#62;-0.&#60;NodeId&#62;.data</tt>
<DD>Файл данных, который содержат записи таблиц.

<DT><tt>BACKUP-&#60;BackupId&#62;.&#60;NodeId&#62;.log</tt>
<DD>Журнал, которые содержат совершенные транзакции.</DL>

<P>Выше &#60;BackupId&#62; задает идентификатор для копии, и &#60;NodeId&#62;
идентификатор узла, создающего файл.</P>

<DL COMPACT><DT><strong>Метаданные</strong>
<DD>Метаданные состоят из определений таблицы. Все узлы имеют точно те же
самые определения таблицы, сохраненные на диске.

<DT><strong>Записи таблицы</strong>
<DD>Записи таблицы сохранены пофрагментно. Каждый фрагмент содержит
заголовок, который описывает, которой таблице принадлежат записи. После
списка записей имеется хвост, который содержит контрольную сумму для записей.
Различные узлы сохраняют различные фрагменты в ходе изготовления копии.

<DT><strong>Совершенные транзакции</strong>
<DD>Перечень совершенных транзакций содержит все транзакции, сделанные в ходе
копирования. Только транзакции на таблицах, сохраненных в копии, будут
сохранены в файле регистрации. Различные узлы в копии сохраняют различные
записи файла регистрации, поскольку они ведут разные фрагменты базы данных.
</DL>

<H4>5.4.2 Использование сервера управления, чтобы создать копию</H4>
<P>Прежде чем начать, удостоверьтесь, что кластер правильно сконфигурирован
для резервного копирования.</P>

<OL><LI>Запустите уцправляющий сервер.
<LI>Выполните команду <code>START BACKUP</code>.
<LI>Сервер управления ответит ``Start of backup ordered''. Это означает, что
сервер управления представил на рассмотрение запрос к кластеру, но еще не
получил никакого ответа.

<LI>Сервер управления ответит ``Backup &#60;BackupId&#62; started'', где
&#60;BackupId&#62; задает уникальный идентификатор для этой специфической
копии. Это будет также сохранено в файле регистрации кластера (если он не
сконфигурирован иначе). Это означает, что кластер получил и обработал запрос.
Это не означает, что копия уже завершена.
<LI>Сервер управления сообщит ``Backup &#60;BackupId&#62; completed'',
когда копия закончена.</OL>

<P>Использование сервера управления, чтобы прервать копию:</P>
<OL><LI>Запустите уцправляющий сервер.
<LI>Выполните команду <code>ABORT BACKUP &#60;BACKUPID&#62;</code>. Здесь
&#60;BackupId&#62; задает идентификатор копии, который был включен в ответ
серверу управления, когда копия начата, то есть в сообщение ``Backup
&#60;BackupId&#62; started''. Идентификатор также сохранен в файле
регистрации кластера (cluster.log).

<LI>Сервер управления ответит ``Abort of backup &#60;BackupId&#62; ordered''.
Это означает, что он представил на рассмотрение запрос кластеру, но не
получил никакого ответа.

<LI>Сервер управления ответит ``Backup &#60;BackupId&#62; has been aborted
reason XYZ''. Это означает, что кластер прервал копию и удалил все к ней
относящееся, включая файлы в файловой системе.</OL>

<P>Обратите внимание, что, если не имеется копии с идентификатором
&#60;BackupId&#62;, выполняющейся на момент попытки прерывания, сервер
управления не будет отвечать ничего.</P>

<H4>5.4.3 Восстановление из резервной копии</H4>
<P>Программа восстановления выполнена как отдельная утилита командной строки.
Она читает файлы, созданные из копии, и вставляет сохраненную информацию в
базу данных. Программа восстановления должна быть выполнена один раз для
каждого набора файлов с резервной копией, то есть столько, сколько работало
узлов базы данных, когда копия создавалась.</P>

<P>Первый раз, когда Вы выполняете программу восстановления, Вы также должны
восстановить метаданные, то есть создать таблицы. Действия программы
восстановления рассматриваются как обращение API к кластеру и, следовательно,
нуждаются в свободном подключении, чтобы соединиться с кластером. Это может
быть проверено через <code>ndb_mgm</code> командой SHOW. Переключатель
<code>-c &#60;connectstring&#62;</code> может использоваться, чтобы указать
узел MGM. Файлы с резервной копией должны присутствовать в каталоге, заданном
как параметр программы восстановления. Копия может быть восстановлена в базу
данных с иной конфигурацией, чем та, из которой резервировались данные.
Например, допустим, что копия (с идентификатором 12) создана на кластере с
двумя узлами базы данных (с идентификаторами узлов 2 и 3), а должна быть
восстановлена на кластере с четырьмя узлами. Программа восстановления должна
быть выполнена два раза (по одному для каждого узла базы данных в кластере,
где копия создавалась, как описано ниже.</P>

<P>Обратите внимание: для быстрого восстановления, данные могут быть
восстановлены паралелльно (если имеется достаточно свободных подключений
API). Обратите внимание, однако, что файлы данных должны всегда
обрабатываться перед файлами регистрации!</P>

<P>Еще обратите внимание: кластер должен иметь пустую базу данных перед
началом процесса восстановления.</P>

<H4>5.4.4 Конфигурации для резервирования</H4>
<P>Имеются четыре параметра конфигурации для резервирования:</P>

<DL COMPACT><DT><strong>BackupDataBufferSize</strong>
<DD>Объем памяти (из общей памяти) используемый, чтобы буферизовать данные
прежде, чем они будут записаны на диск.

<DT><strong>BackupLogBufferSize</strong>
<DD>Объем памяти (из общей памяти) используемый, чтобы буферизовать записи
файла регистрации прежде, чем они будут записаны на диск.

<DT><strong>BackupMemory</strong>
<DD>Общая память, распределенная в узле базы данных для изготовления копии.
Это должно быть суммой памяти, распределенной для двух буферов.

<DT><strong>BackupWriteSize</strong>
<DD>Размер блоков, записанных на диск. Это совместно применяется к буферам
данных и файлов регистрации.</DL>

<H4>5.4.5 Проблемы резервирования</H4>
<P>Если при выдаче запроса на резервирование возвращен код ошибки, то
проверьте, что имеется достаточно памяти, распределенной для копирования (то
есть, параметры конфигурации). Также проверьте, что имеется достаточно
пространства в разделе жесткого диска-адресате.</P>

<A name="intercon"><H2>6 Использование высокоскоростных
межсоединений в MySQL Cluster</H2>
<P>Уже перед разработкой NDB Cluster в 1996 было очевидно, что одной из
главных проблем формирования параллельных баз данных является связь между
узлами в сети. Таким образом, с самого начала NDB Cluster был разработан с
концепцией транспортера, чтобы учесть различные сетевые среды.</P>

<P>В настоящее время основание кода включает 4 различных транспортера, где 3
из них в настоящее время работают. Большинство пользователей сегодня
использует TCP/IP через Ethernet, так как это существует на всех машинах. Это
также наиболее проверенный транспортер в MySQL Cluster.</P>

<P>Для пользователей, которые желают максимальной эффективности, возможно
использовать быстрые связи в кластере. Имеются два способа достигнуть этого:
может быть разработан транспортер, чтобы обработать этот случай, или можно
использовать реализации сокетов, которые обходят стек TCP/IP.</P>

<P>Авторы пакета сделали некоторые эксперименты с обоими вариантами,
использующими технологию SCI, разработанную Dolphin (www.dolphinics.no).</P>

<H3>6.1 Настройка MySQL Cluster для использования сокетов SCI</H3>
<P>В этом разделе мы покажем, как можно использовать кластер,
сконфигурированный для нормальной связи через TCP/IP, чтобы взамен
использовать сокеты SCI. Эта документация основана на сокетах SCI (они также
часто упоминаются как SCI Socket) версии 2.3.0 от 1 октября 2004.</P>

<P>Для использования сокетов SCI можно использовать любую версию MySQL
Cluster. Тесты выполнялись на версии 4.1.6. Никакой специальной версии не
компилируется, так как это использует нормальные обращения сокета, что
является нормальной установкой конфигураций для MySQL Cluster. SCI Sockets
в настоящее время обеспечиваются только ядрами Linux 2.4 и 2.6.
SCI-транспортеры работает на большем количестве OS, хотя
проверена только Linux 2.4.</P>

<P>Имеются по существу четыре вещи, необходимые, чтобы запустить сокеты SCI.
Сначала необходимо сформировать библиотеки SCI. Затем SCI-библиотеки ядра
должны быть установлены. Далее один или два файла конфигурации должны быть
установлены. Наконец, SCI-библиотека ядра должна быть включена для всей
машины или хотя бы для оболочки, где процессы запущены процессы MySQL
Cluster. Этот процесс должен быть повторен для каждой машины в кластере,
который будет использовать сокеты SCI.</P>

<P>Два пакета должны быть установлены, чтобы получить работающие сокеты SCI.
Первый пакет формирует библиотеки, которые собственно работают с сокетами, а
второй интерфейс сокетов с ОС. В настоящее время они доступны только в
формате исходного текста.</P>

<P>Последние версии этих пакетов в настоящее время могут быть найдены на
http://www.dolphinics.no/support/downloads.html.

<P>Версии, описанные здесь:
<PRE>
http://www.dolphinics.no/ftp/source/DIS_GPL_2_5_0_SEP_10_2004.tar.gz
http://www.dolphinics.no/ftp/source/SCI_SOCKET_2_3_0_OKT_01_2004.tar.gz
</PRE></P>

<P>Следующий шаг должен распаковать архивы, SCI Sockets распакован ниже
уровня кода DIS. Затем основание кода компилируется. Пример ниже показывает
команды, используемые в Linux/x86, чтобы выполнить это.
<PRE>
shell&#62; tar xzf DIS_GPL_2_5_0_SEP_10_2004.tar.gz
shell&#62; cd DIS_GPL_2_5_0_SEP_10_2004/src/
shell&#62; tar xzf ../../SCI_SOCKET_2_3_0_OKT_01_2004.tar.gz
shell&#62; cd ../adm/bin/Linux_pkgs
shell&#62; ./make_PSB_66_release
</PRE></P>

<P>Если Вы используете AMD64, чтобы использовать 64-разрядные расширения,
надо использовать make_PSB_66_X86_64_release вместо make_PSB_66_release. При
работе на Itanium соответственно вызовите make_PSB_66_IA64_release.
Вариант X86-64 должен работать и на Intel EM64T, но никакие известные тесты
этого не проводились.</P>

<P>После формирования основания кода, он должен быть помещен в tar-архив с
именем, отражающим DIS, OS и дату. Теперь настало время, чтобы установить
пакет в соответствующее место. В этом примере мы поместим установку в каталог
/opt/DIS. Эти действия наиболее вероятно требуют, чтобы Вы вошли в систему
как пользователь root:
<PRE>
shell&#62; cp DIS_Linux_2.4.20-8_181004.tar.gz /opt/
shell&#62; cd /opt
shell&#62; tar xzf DIS_Linux_2.4.20-8_181004.tar.gz
shell&#62; mv DIS_Linux_2.4.20-8_181004 DIS
</PRE></P>

<P>Теперь, когда все библиотеки находятся в соответствующем месте, мы должны
гарантировать, что SCI-платы получают соответствующие идентификаторы внутри
адресного пространства SCI. Поскольку SCI работает с сетями на низком уровне,
сначала необходимо остановиться на сетевой структуре.</P>

<P>Имеются три типа сетевых структур. Первый простое одномерное кольцо,
второй использует коммутаторы SCI с одним кольцом на порт коммутатора и в
заключение имеется 2D/3D тор. Каждый вариант имеет свой стандарт
обеспечения идентификаторов узла.</P>

<P>Простое кольцо использует идентификаторы узла через 4:
<PRE>
4, 8, 12, ....
</PRE>

<P>Следующая возможность использует коммутатор. SCI-коммутатор имеет 8
портов, причем каждый порт может обрабатывать кольцо. Здесь необходимо
гарантировать, что кольца на коммутаторе используют различные наборы
идентификатора узла. Так что первый порт использует идентификаторы узла ниже
64, следующие 64 идентификатора узла распределены для второго порта и т.д.:
<PRE>
4,8, 12, ... , 60  Кольцо на порте 1
68, 72, .... , 124 Кольцо на порте 2
132, 136, ..., 188 Кольцо на порте 3
..
452, 456, ..., 508 Кольцо на порте 8
</PRE>

<P>2D/3D тор в сетевой структуре принимает во внимание, где каждый узел
находится в каждой размерности, добавляя 4 для каждого узла в первой
размерности, 64 во второй и 1024 в третьей размерности.</P>

<P>В нашем тестировании мы использовали коммутаторы. Большинство
действительно больших инсталляций кластера использует торы. Свойство
дополнительного пространства, которое обеспечивают коммутаторы в том, что с
двойными SCI-платами и соответственно коммутаторами можно легко сформировать
избыточную сеть, где потери времени в SCI-сети составят около 100
микросекунд. Это свойство обеспечивается SCI-транспортером и в настоящее
время также разработано для реализации сокетов SCI.</P>

<P>Потери времени для торов также возможны, поскольку эта схема сети требует
отправки новых индексов маршрутизации всем узлам. Задержка составляет около
100 миллисекунд, что обычно вполне допустимо.</P>

<P>Помещая NDB-узлы в соответствующих местах в архитектуре, возможно
использовать пару коммутаторов, чтобы формировать структуру, где 16
компьютеров могут быть взаимосвязаны, и никакой одиночный сбой не сможет
препятствовать больше, чем один компьютеру. С 32 компьютерами и 2
коммутаторами можно сконфигурировать кластер таким способом, что никакой
одиночный сбой не сможет препятствовать больше, чем двум узлам, и в этом
случае также известно, которая пара повреждена. Таким образом, помещая узлы
в отдельных группах NDB-узлов можно сформировать безопасную
установку MySQL Cluster.</P>

<P>Чтобы установить идентификатор узла SCI-платы, используйте следующую
команду, которая находится в каталоге <code>/opt/DIS/sbin</code>. Здесь
-c 1 относится к количеству SCI-плат в машине, 1 применяется только, если
1 плата находится в машине. В этом случае всегда используйте адаптер 0 (-a
0). 68 задает идентификатор узла в этом примере:
<PRE>
shell&#62; ./sciconfig -c 1 -a 0 -n 68
</PRE>

<P>В случае, если Вы имеете несколько SCI-плат в вашей машине, надо понять,
какая плата соответствует каждому номеру адаптера. Для этого скомвндуйте:
<PRE>
shell&#62; ./sciconfig -c 1 -gsn
</PRE>

<P>Это даст серийный номер, который может быть найден на задней части
SCI-платы и на плате непосредственно. Повторите это затем для -c 2 и так
далее для всех плат. Это идентифицирует, какая карта какому id соответствует.
Затем установите идентификаторы узла для всех плат.</P>

<P>Теперь мы установили необходимые библиотеки и двоичные модули. Мы также
установили SCI-идентификаторы узла. Следующий шаг должен установить
отображение имен машин (или адресов IP) к SCI-идентификаторам узла.</P>

<P>Файл конфигурации для сокетов SCI должен быть помещен в
<code>/etc/sci/scisock.conf</code>. Этот файл содержит отображение имен (или
IP) на SCI-идентификаторы узлов. SCI-идентификатор узла отобразится на имя,
чтобы связаться через соответствующую SCI-плату. Ниже показан очень простой
такой файл конфигурации:
<PRE>
#host           #nodeId
alpha           8
beta            12
192.168.10.20   16
</PRE>

<P>Также возможно ограничить эту конфигурацию, чтобы опрашивать только некое
подмножество портов этих машин. Чтобы сделать это, используется другая
конфигурация, которая помещена в <code>/etc/sci/scisock_opt.conf</code>:
<PRE>
#-key                        -type        -values
EnablePortsByDefault                      yes
EnablePort                  tcp           2200
DisablePort                 tcp           2201
EnablePortRange             tcp           2202 2219
DisablePortRange            tcp           2220 2231
</PRE></P>

<P>Теперь мы готовы установить драйверы. Мы должны сначала установить
драйверы низкого уровня, а уж затем драйвер SCI-сокетов:
<PRE>
shell&#62; cd DIS/sbin/
shell&#62; ./drv-install add PSB66
shell&#62; ./scisocket-install add
</PRE></P>

<P>Если желательно, можно теперь проверить установку, вызывая скрипт, который
проверяет, что все узлы в файлах конфигурации сокетов SCI доступны:
<PRE>
shell&#62; cd /opt/DIS/sbin/
shell&#62; ./status.sh
</PRE></P>

<P>Если Вы обнаруживаете ошибку и должны изменить файлы конфигурации SCI,
затем необходимо использовать программу ksocketconfig,
чтобы сменить конфигурацию.
<PRE>
shell&#62; cd /opt/DIS/util
shell&#62; ./ksocketconfig -f
</PRE></P>

<P>Чтобы проверить, что сокеты SCI фактически используются, Вы можете
использовать программу <code>latency_bench</code>, которая должна иметь
клиентский и серверный компоненты, чтобы проверить время ожидания. Прежде,
чем Вы используете эту программу, Вы также должны установить переменную
LD_PRELOAD, как показано ниже.</P>

<P>Установите сервер командой:
<PRE>
shell&#62; cd /opt/DIS/bin/socket
shell&#62; ./latency_bench -server
</PRE>

<P>Теперь запустите клиент:
<PRE>
shell&#62; cd /opt/DIS/bin/socket
shell&#62; ./latency_bench -client hostname_of_server
</PRE>

<P>Теперь конфигурация SCI завершена. MySQL Cluster готов использовать
совместно сокеты и транспортер SCI.</P>

<P>Следующий шаг к запуску MySQL Cluster. Чтобы включить использование
сокетов SCI, необходимо установить системную переменную LD_PRELOAD перед
стартом ndbd, mysqld и ndb_mgmd, чтобы те использовали сокеты SCI. Переменная
LD_PRELOAD должна указывать на ядерную библиотеку для SCI Sockets.</P>

<P>Например, запустим ndbd в оболочке bash.
<PRE>
bash-shell&#62; export LD_PRELOAD=/opt/DIS/lib/libkscisock.so
bash-shell&#62; ndbd
</PRE></P>

<P>Для tcsh команда чуть иная:
<PRE>
tcsh-shell&#62; setenv LD_PRELOAD=/opt/DIS/lib/libkscisock.so
tcsh-shell&#62; ndbd
</PRE></P>

<P>Заметьте, что MySQL Cluster может использовать только ядерный
вариант SCI Sockets.</P>

<H3>6.2 Эталонные тесты низкого уровня, чтобы понять
воздействие связей в кластере</H3>
<P>Процесс ndbd имеет ряд простых конструкций, которые используются, чтобы
обратиться к данным в MySQL Cluster. Есть очень простой эталонный тест, чтобы
проверить эффективность каждой такой инструкции и эффект, который различные
типы связи в кластере оказывают на их эффективность.</P>

<P>Имеются четыре метода доступа:
<DL COMPACT><DT><code>Primary key access</code>
<DD>Это простой доступ к одной записи через первичный ключ. В самом простом
случае одновременно обращаются только к одной записи. Это означает, что
полная стоимость установки TCP/IP-сообщения и ряд издержек для переключения
контекста принимается этим одиночным запросом. В пакетном случае, где,
например, 32 доступа к первичному ключу представлены одним пакетом, эти 32
доступа совместно используют стоимость установки TCP/IP-сообщений и
контекстных переключений (если TCP/IP для различных адресатов, естественно,
ряд TCP/IP-сообщений должен быть установлен).

<DT><code>Unique key access</code>
<DD>Доступ к уникальному ключу очень похож на доступ через первичный ключ за
исключением того, что он выполнен как чтение индексной таблицы,
сопровождаемым доступом через первичный ключ на таблице. Однако, только один
запрос будет послан серверу MySQL, чтение индексной таблицы обработано
процессом ndbd. Таким образом, эти запросы извлекают пользу
из пакетной обработки.

<DT><code>Full table scan</code>
<DD>Когда никакие индексы не существуют для поиска на таблице, выполняется
полный просмотр таблицы. Это один запрос к процессу ndbd, который делит
просмотр таблицы на набор параллельных просмотров на всех процессах ndbd в
кластере. В будущих версиях MySQL сервер сможет использовать фильтры
при таких просмотрах.

<DT><code>Range scan using ordered index</code>
<DD>Когда используется упорядоченный индекс, это выполнит просмотр тем же
самым способом, как полный просмотр таблицы за исключением того, что это
просмотрит только те записи, которые находятся в диапазоне, используемом
установкой запросов сервера MySQL. В будущих версиях специальная оптимизация
гарантирует, что, когда все связанные индексные атрибуты включают все
атрибуты в отдельный ключ, только один раздел будет просмотрен вместо всех
сразу в параллельном режиме.</DL>

<P>Чтобы проверить основную эффективность этих методов доступа, разработан
набор эталонных тестов. Один такой эталонный тест, testReadPerf, использует
простые и пакетные доступы через первичный и уникальный ключи. Эталонный тест
также измеряет стоимость установки диапазона просмотра, возврат одиночной
записи и в заключение имеется вариант, который использует просмотр диапазона,
чтобы выбрать пакет записей.</P>

<P>Этим способом мы можем проверять стоимость выдачи одиночного доступа к
ключу, одиночного доступа для просмотра записи и измерять воздействие
реализации средств связи на эти основные методы доступа.</P>

<P>Мы выполнили тот основной эталонный тест, используя нормальный транспортер
через сокеты TCP/IP и повторили его через сокеты SCI. Ниже приведены
результаты для запроса 20 записей за доступ. Различие между последовательным
и пакетным доступами понижается коэффициентом 3-4 при использовании записей
2kB. SCI-сокеты не тестировались с записями в 2 kB. Тесты выполнялись на
кластере с 2 узлами с 2 CPU AMD MP1900+ на каждом.
<PRE>
Тип доступа:         Сокеты TCP/IP            Сокеты SCI
Serial pk access:    400 microseconds         160 microseconds
Batched pk access:    28 microseconds          22 microseconds
Serial uk access:    500 microseconds         250 microseconds
Batched uk access:    70 microseconds          36 microseconds
Indexed eq-bound:   1250 microseconds         750 microseconds
Index range:          24 microseconds          12 microseconds
</PRE></P>

<P>Мы делали также другой набор тестов, чтобы проверить эффективность сокетов
SCI, сравниваемых с использованием SCI-транспортера, и все это в сравнении с
TCP/IP-транспортером. Все эти тесты использовали доступ через первичный ключ
последовательно, многопоточно, а также одновременно многопоточно и пакетно.
</P>

<P>Более или менее все эти тесты показали, что сокеты SCI были приблизительно
на 100% быстрее TCP/IP. Транспортер SCI в большинстве случаев был быстрее
сокетов SCI. Один известный случай с многими потоками в программе теста
показал, что SCI-транспортер вел себя очень плохо, если
использовался в процессе mysqld.</P>

<P>Таким образом, для большинства эталонных тестов сокеты SCI улучшают
эффективность примерно вдвое, относительно TCP/IP за исключением редких
случаев, когда эффективность связи не (проблема типа того, когда фильтры
просмотра занимают большинство времени обработки, или когда нужны очень
большие пакеты доступа через первичные ключи. В таком случае обработка CPU в
процессах ndbd становится довольно большой частью стоимости.</P>

<P>Использование SCI-транспортера вместо SCI-сокетов представляет интерес
только в связи между процессами ndbd. Использование SCI-транспортера также
представляет интерес только, если CPU может быть выделен для процесса ndbd,
поскольку SCI-транспортер гарантирует, что ndbd никогда не будет
бездействовать. Также важно гарантировать, что ndbd имеет приоритет,
установлен таким способом, что процесс не теряет в приоритете из-за работы
длительное время (как может быть выполнено, блокируя процессы на CPU в Linux
2.6). Если это возможная конфигурация, процесс ndbd принесет пользы на 10-70%
больше по сравнению с использованием сокетов SCI при выполнении модификаций
и, вероятно, также на параллельных действиях просмотра.</P>

<P>Имеются несколько других реализаций оптимизированных вариантов сокетов для
кластеров, рассмотренных в различных статьях. Они включают оптимизированные
варианты для Myrinet, Gigabit Ethernet, Infiniband и VIA interface.
Разработчики пока проверили MySQL Cluster только на сокетах SCI.</P>

<A name="limit"><H2>7 Ограничения MySQL Cluster в версии 4.1</H2>
<P>Ниже приведен список известных ограничений версии 4.1 в сравнении со
способами хранения MyISAM и InnoDB. В настоящее время не имеется никаких
планов по их снятию в версиях ряда 4.1 (но в 5.0 или позднее они непременно
будут устранены). На <a HREF="../../../bugs.mysql.com/default.htm">http://bugs.mysql.com
</a>, в категории cluster, находится список известных глюков, подлежащих
ликвидации в версиях серии 4.1.</P>

<DL COMPACT><DT>Расхождения в синтаксисе (приводящие к ошибкам при выполнении
существующей прикладной программы).

<DD><UL><LI>Не все наборы символов поддерживаются.
<LI>Никакие префиксные индексы не работают (можно индексировать
только полные поля).
<LI>Никакие текстовые индексы не поддерживаются.</UL>

<DT>Расхождения в ограничениях и поведении (могут приводить к ошибке при
выполнении существующей прикладной программы)
<DD><UL><LI>Отсутствует частичная обратная перемотка транзакций при ошибке. В
результате, например, ошибка дублирования ключа, приведет к обратной
перемотке целой транзакции.

<LI>Есть несколько жестких ограничений, которые являются настраиваемыми, но
сдерживаются доступной оперативной памятью кластера. Большинство параметров
конфигурации могут быть расширены интерактивно.
<UL><LI>Размер памяти базы данных и индексов, <code>DataMemory</code> и
<code>IndexMemory</code> не резиновые.

<LI>Насколько большие транзакции могут выполняться, установлено параметром
конфигурации <code>MaxNoOfConcurrentOperations</code> (загрузка данных,
усечение и изменение таблицы обработаны особенно, выполняя несколько
транзакций, и, таким образом, не имеет этого ограничения).

<LI>Различные ограничения связаны с таблицами и индексами, например,
максимальное число упорядоченных индексов, <code>MaxNoOfOrderedIndexes</code>
и так далее в этом же направлении.</UL>

<LI>Имена баз данных, таблиц и атрибутов не могут быть в ndb-таблицах столь
длинными, как в других драйверах таблицы. Имена атрибутов будут усечены до 31
символам, и если они после этого окажутся не уникальными, Вы получите ошибку.
Имя базы данных и имя таблицы суммарно могут быть максимум 122 символа.

<LI>Максимальное количество метаобъектов данных ограничено 1600 (это включает
таблицы, системные таблицы, индексы и BLOB-объекты).
<LI>Максимальное число атрибутов на таблицу, ограничено 128.
<LI>Размер строки максимум 8k (не включая BLOB-конструкции).
<LI>Максимальное число атрибутов в ключе равно 32.</UL>

<DT>Не обеспечиваемые свойства (это не ошибки, но не все равно неприятно).
<DD><UL><LI>Конструкция внешнего ключа игнорируется (то же самое поведение
имеется и у драйвера MyISAM).
<LI>Точки сохранения и обратная перемотка к точке сохранения игнорируются
(то же самое поведение имеется и у драйвера MyISAM).</UL>

<DT>Эффективность и связанные ограничения
<DD><UL><LI>Заблокирован кэш запросов, так как он будет ошибочен, если
модификация происходит на другом сервере MySQL.
<LI>Эффективность запроса падает из-за последовательного доступа к серверам,
распределенно хранящим данные.

<LI>Записи в статистическом диапазоне не обеспечиваются, что дает не
оптимальное планирование запросов в некоторых случаях. Используйте
конструкцию <code>USE INDEX</code> или <code>FORCE INDEX</code>, чтобы не
оптимальные планы запросов.

<LI>Уникальный хэш-индекс (созданный с помощью <code>USING HASH</code>)
не может использоваться для доступа к таблице, если NULL задан
как часть ключа.</UL>

<DT>Отсутствующие свойства системы.
<DD><UL><LI>Есть поддержка только для уровня изоляции READ_COMMITTED (InnoDB
поддерживает READ_COMMITTED, REPEATABLE_READ и SERIALIZABLE).
<LI>Продолжительные транзакции на диск не сбрасываются (репликация работает,
но это, увы, не гарантирует, что файлы регистрации непременно будут сброшены
на диск при завершении транзакции).</UL>

<DT>Проблемы с несколькими серверами MySQL (не связаны с MyISAM или InnoDB).
<DD><UL><LI>Изменение таблицы не полностью блокирует ее при выполнении
нескольких серверов MySQL (отсутствует распределенная блокировка таблицы).
<LI>MySQL Replication не будет работать правильно, если модификации выполнены
на нескольких серверах MySQL. Если схема разделения базы данных выполнена в
прикладном уровне, и никакие транзакции не происходят поверх этих разделов,
это будет работать.</UL>

<DT>Проблемы кластера (не связаны с MyISAM или InnoDB).
<DD><UL><LI>Все машины в кластере должны использовать ту же самую
архитектуру, то есть все машины, являющиеся узлами, должны оперировать
числами в формате big-endian или little-endian, и Вы не можете использовать
смесь обоих. Например, Вы не можете иметь узел управления, выполняющийся на
PPC, который направляет работу сервера на x86-машине. Это ограничение не
относится к машинам, просто выполняющим клиенты <code>mysql</code>, которые
могут обращаться к кластеру.

<LI>Нет интерактивной схемы изменения (alter table, Ndb create index).
<LI>Нельзя интерактивно добавлять или удалять узлы в кластере.
<LI>При использовании нескольких серверов управления нужно дать узлам
идентификатор явно в строках подключения, так как автоматическое
распределение идентификаторов узлов не работает для нескольких серверов.

<LI>При использовании нескольких серверов управления нужно иметь ту же самую
конфигурацию на всех серверах управления. Нет проверки этого условия.
<LI>Максимальное количество узлов памяти: 48.
<LI>Общий максимальное число узлов: 63 (серверы MySQL, узлы памяти и
серверы для управления).</UL></DL>

</index></td><td width="20%">
<script type="text/javascript">
    var begun_auto_colors           = new Array();
    var begun_auto_fonts_size       = new Array();
    var begun_auto_pad              =       97517308;     // идентификатор площадки
    var begun_auto_limit            =              5;     // число объявлений выводимых на площадке
    var begun_auto_width            =            250;     // ширина блока объявлений
    begun_auto_colors[0]            =      '#0000CC';     // цвет ссылки объявлений
    begun_auto_colors[1]            =      '#000000';     // цвет текста объявления
    begun_auto_colors[2]            =      '#00CC00';     // цвет домена объявления
    begun_auto_colors[3]            =      '#FFFFFF';     // цвет фона блока объявлений
    begun_auto_fonts_size[0]        =          '9pt';     // р-мер шрифта ссылки объявлений
    begun_auto_fonts_size[1]        =          '9pt';     // р-мер шрифта текста объявления
    begun_auto_fonts_size[2]        =          '8pt';     // р-мер шрифта домена объявления
    begun_auto_fonts_size[3]        =          '8pt';     // р-мер шрифта заглушки
    var begun_block_type            =     'Vertical';     // тип блока
    var begun_rambler_type          =              1;     // цвет блока поиска Рамблер
    begun_koi8 = 1;
</script>
<script src="../../../autocontext.begun.ru/autocontext.js"
type="text/javascript"></script>
</td></tr></table>

<p><table><tr><td>
<form method="get" name="fform" onSubmit="fsearch(strf.value);
return false;">
<input type="hidden" name="clid" value="39177">
<b>Поиск</b><table><tr>
<td><input type="text" name="strf" size="15" style="font-size: 9pt" /></td></tr>
<tr><td><font size="-1"><input type="radio" name="myradio" value="0"
checked id="at_site"/>
<label for="at_site">На сайте</label><br/>
<input type="radio" name="myradio" value="1" id="at_ya">
<label for="at_ya">В Яндексе</label></font></td></tr>

<tr><td><input type="submit" value="Найти" style="font-size: 9pt"></td></tr>
</table></form></td>
<td>&nbsp;</td><td><script language="JavaScript"
src="../../../b190.takru.com/in.php@id=199275">
</script></td></tr></table></p>

<p><center><table><tr><td><font size="+1"><B>Найди своих коллег!</B></font><BR>
<script language="javascript"
src="../../../rldp.lovemesweet.ru/banner/db.js.php@rows=01&cols=05&bg=33FF33&sex=0&afrom=18&ato=99&headgif=&sf=1&nwnd=1&pmin=0&cid=RU&tid=0&smu=0&tcl1=000000&tcl2=FF0000">
</script></td>

<td><iframe src="../../../www.linuxcenter.ru/trans/list.phtml@ref=121965&n=5&price=yes"
frameborder="0" vspace="0" hspace="0" width="300" height="350" marginwidth="0"
marginheight="0" scrolling="no"></iframe></td></tr></table></center></p>

<script type="text/javascript" src="../../../bin-layer.ru/popup-686-1.js">
</script>


</BODY>
</html>
